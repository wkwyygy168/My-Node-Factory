import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def fetch_and_extract(url):
    """æè‡´å…¼å®¹æŠ“å–ï¼šå°è¯•é•œåƒåŠ é€Ÿ + æš´åŠ›è§£ç """
    # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œé˜²æ­¢è¢«æºç«™æ‹‰é»‘
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
    
    # è‡ªåŠ¨å°† GitHub é“¾æ¥è½¬æ¢ä¸º CDN é•œåƒï¼Œç»•è¿‡ GitHub Actions çš„è®¿é—®å¢™
    alt_url = url
    if "raw.githubusercontent.com" in url:
        try:
            parts = url.split('/')
            if len(parts) >= 6:
                user, repo, branch, path = parts[3], parts[4], parts[5], "/".join(parts[6:])
                alt_url = f"https://fastly.jsdelivr.net/gh/{user}/{repo}@{branch}/{path}"
        except: pass

    # å°è¯•åŒè·¯æŠ“å–ï¼šå…ˆé•œåƒï¼ŒååŸå§‹
    for target in [alt_url, url]:
        try:
            r = requests.get(target, headers=headers, timeout=15)
            if r.status_code == 200 and len(r.text) > 10:
                content = r.text.strip()
                # åè®®æŒ‡çº¹è¯†åˆ«
                pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
                
                # 1. æŠ“å–æ˜æ–‡
                nodes = re.findall(pattern, content, re.I)
                
                # 2. æš´åŠ› Base64 è§£ç  (è¡¥å…¨å¡«å……ç¬¦å¹¶å¿½ç•¥éæ ‡å‡†å­—ç¬¦)
                try:
                    b64_str = re.sub(r'[^a-zA-Z0-9+/=]', '', content)
                    missing_padding = len(b64_str) % 4
                    if missing_padding: b64_str += "=" * (4 - missing_padding)
                    decoded = base64.b64decode(b64_str).decode('utf-8', errors='ignore')
                    nodes.extend(re.findall(pattern, decoded, re.I))
                except: pass
                
                if nodes: return nodes
        except: continue
    return []

def collector():
    print("ğŸš€ [SYSTEM] å¼•æ“ V13.0ï¼šå¯åŠ¨å…¨çƒ CDN é•œåƒçˆ†ç ´æ¨¡å¼...")
    # è¿™é‡Œä¿æŒä½ é‚£ 80 æ¡ targets ä¸å˜
    targets = [
        # ... è¯·ä¿æŒä½ ä»£ç ä¸­é‚£ 80 æ¡æºé“¾æ¥ ...
    ]

    all_found = []
    # ä½¿ç”¨ 25 çº¿ç¨‹ï¼Œå…¼é¡¾æŠ“å–æ•ˆç‡ä¸ç¨³å®šæ€§
    with ThreadPoolExecutor(max_workers=25) as executor:
        results = executor.map(fetch_and_extract, targets)
        for res in results:
            if res: all_found.extend(res)

    unique_nodes = list(set(all_found))
    with open("nodes.txt", "w", encoding="utf-8") as f:
        # å¦‚æœæŠ“åˆ° 1 ä¸ªä»¥ä¸Šçš„èŠ‚ç‚¹ï¼Œå°±æ­£å¸¸å†™å…¥
        if len(unique_nodes) > 1:
            f.write("\n".join(unique_nodes))
            print(f"âœ… [SUCCESS] çˆ†ç ´å®Œæˆï¼æˆåŠŸæ”¶å‰²èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
        else:
            # ä¿®æ”¹ä¿åº•èŠ‚ç‚¹ï¼Œæ˜ç¡®æç¤º
            f.write("ss://YWVzLTI1Ni1jZmI6WG44aktkbURNMDBJZU8lIyQjZkpBTXRzRUFFVU9wSC9ZV1l0WXFERm5UMFNWQDEwMy4xODYuMTU1LjI3OjM4Mzg4#äº‘ç«¯æ”¶å‰²æœºæ­£åœ¨å…¨åŠ›ä½œä¸š_è¯·ç¨ååˆ·æ–°")

if __name__ == "__main__":
    collector()
