import requests
import re
import base64
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

def fetch_and_decode(url):
    """å…¨å¹³å°æš´åŠ›æ”¶å‰²ï¼šæ”¯æŒæ˜æ–‡ã€Base64ã€è®¢é˜…æ ¼å¼"""
    headers = {'User-Agent': 'clash.meta'}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            content = r.text.strip()
            pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
            found = re.findall(pattern, content, re.I)
            try:
                for _ in range(2):
                    missing_padding = len(content) % 4
                    if missing_padding: content += "=" * (4 - missing_padding)
                    content = base64.b64decode(content).decode('utf-8', errors='ignore')
                    found.extend(re.findall(pattern, content, re.I))
            except: pass
            return found
    except: return []

def get_dynamic_urls():
    """åŠ¨æ€æ—¥æœŸæ”¶å‰²é€»è¾‘"""
    dynamic_list = []
    today = datetime.now()
    for i in range(5):
        t = today - timedelta(days=i)
        d_str, m_str, y_str = t.strftime("%Y%m%d"), t.strftime("%m"), t.strftime("%Y")
        dynamic_list.append(f"https://node.nodefree.me/{y_str}/{m_str}/{d_str}.txt")
        dynamic_list.append(f"https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/{d_str}.txt")
    return dynamic_list

def collector():
    print("ğŸš€ [FACTORY] å¼€å¯å·¥ä¸šåŒ–æ”¶å‰²æ¨¡å¼ï¼Œæ­£åœ¨æ³¨å…¥è‡ªå®šä¹‰åç¼€...")
    targets = [
        *get_dynamic_urls(),
        "https://raw.githubusercontent.com/shuaidaoya/FreeNodes/main/nodes.txt",
        "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/nodes.txt",
        "https://raw.githubusercontent.com/vpei/Free-Node-Merge/main/o/node.txt",
        "https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/snippets/nodes.txt",
        "https://t.me/s/v2rayfree",
        "https://t.me/s/V2List",
        "https://raw.githubusercontent.com/awesome-vpn/vpn/master/free.txt",
        "https://raw.githubusercontent.com/mianfeifq/share/main/data.txt"
    ]

    all_found = []
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = executor.map(fetch_and_decode, list(set(targets)))
        for res in results:
            if res: all_found.extend(res)

    unique_nodes = list(set(all_found))
    
    # --- æ ¸å¿ƒæ”¹è¿›ï¼šæ‰¹é‡æ³¨å…¥åç¼€ ---
    tagged_nodes = []
    suffix = "youtube@å…è´¹å¼€æº"
    for node in unique_nodes:
        # å¦‚æœèŠ‚ç‚¹åŸæœ¬å°±æœ‰å¤‡æ³¨ï¼ˆå¸¦#ï¼‰ï¼Œæˆ‘ä»¬æŠŠå®ƒæ›¿æ¢æ‰æˆ–åŠ åœ¨åé¢
        if "#" in node:
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå»æ‰åŸæœ‰çš„å¤‡æ³¨ï¼Œæ¢æˆè€å¤§çš„ä¸“å±å¤‡æ³¨
            clean_node = node.split("#")[0]
            tagged_nodes.append(f"{clean_node}#{suffix}")
        else:
            tagged_nodes.append(f"{node}#{suffix}")

    with open("nodes.txt", "w", encoding="utf-8") as f:
        if len(tagged_nodes) > 100:
            f.write("\n".join(tagged_nodes))
            print(f"âœ… [SUCCESS] å…¨ç½‘æ”¶å‰²å®Œæ¯•ï¼å·²ä¸º {len(tagged_nodes)} ä¸ªèŠ‚ç‚¹æ³¨å…¥ä¸“å±åç¼€ã€‚")
        else:
            f.write(f"ss://YWVzLTI1Ni1jZmI6WG44aktkbURNMDBJZU8lIyQjZkpBTXRzRUFFVU9wSC9ZV1l0WXFERm5UMFNWQDEwMy4xODYuMTU1LjI3OjM4Mzg4#{suffix}")

if __name__ == "__main__":
    collector()
