import requests
import re
import base64
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

def fetch_nodes(url):
    """æœ€å¼ºå…¼å®¹æŠ“å–ï¼šåªè¦æœ‰æµé‡ï¼Œç»Ÿç»ŸæŠ“å›æ¥"""
    # æ¨¡æ‹Ÿå¤šç§ UAï¼Œé˜²æ­¢è¢«æºç«™å±è”½
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': '*/*'
    }
    # åŠ å…¥ GitHub é•œåƒåŠ é€Ÿï¼Œæé«˜ Actions æŠ“å–æˆåŠŸç‡
    mirrors = [url, f"https://ghproxy.net/{url}", f"https://mirror.ghproxy.com/{url}"]
    
    for target in mirrors:
        try:
            r = requests.get(target, headers=headers, timeout=12)
            if r.status_code == 200:
                content = r.text.strip()
                # è¦†ç›–æ‰€æœ‰ä¸»æµåè®®ï¼šSS, SSR, Vmess, Vless, Trojan, Hysteria2, Tuic
                pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
                found = re.findall(pattern, content, re.I)
                
                # å°è¯• 3 å±‚ Base64 æ·±åº¦è§£ç ï¼Œå‹æ¦¨éšè—èŠ‚ç‚¹
                tmp = content
                for _ in range(3):
                    try:
                        missing = len(tmp) % 4
                        if missing: tmp += "=" * (4 - missing)
                        decoded = base64.b64decode(tmp).decode('utf-8', errors='ignore')
                        found.extend(re.findall(pattern, decoded, re.I))
                        tmp = decoded
                    except: break
                return found
        except: continue
    return []

def get_huge_sources():
    """æµ·é‡æºåˆ—è¡¨ï¼šç»“åˆ Barabama, shuaidaoya åŠå…¨ç½‘èšåˆ"""
    sources = []
    today = datetime.now()
    # åŠ¨æ€æŠ“å–æœ€è¿‘ 7 å¤©çš„å½’æ¡£ï¼Œä¿è¯æ•°é‡å¤Ÿå¤§
    for i in range(7):
        t = today - timedelta(days=i)
        d, m, y = t.strftime("%Y%m%d"), t.strftime("%m"), t.strftime("%Y")
        sources.append(f"https://node.nodefree.me/{y}/{m}/{d}.txt")
        sources.append(f"https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/{d}.txt")
    
    # åŠ å…¥æ›´å¤šé«˜äº§é‡çš„èšåˆæº
    extras = [
        "https://raw.githubusercontent.com/shuaidaoya/FreeNodes/main/nodes.txt",
        "https://raw.githubusercontent.com/vpei/Free-Node-Merge/main/o/node.txt",
        "https://raw.githubusercontent.com/snakem982/proxypool/main/source/all.txt",
        "https://raw.githubusercontent.com/mizero/FreeNode/main/nodes.txt",
        "https://raw.githubusercontent.com/tjm022/Free-Node-Merge/main/node.txt",
        "https://raw.githubusercontent.com/Anaer/Sub/master/v2ray.txt",
        "https://raw.githubusercontent.com/LonUp/NodeList/main/NodeList",
        "https://t.me/s/v2rayfree",
        "https://t.me/s/V2List",
        "https://t.me/s/free_v2ray_config"
    ]
    return list(set(sources + extras))

def collector():
    print("ğŸš€ [MASSIVE-COLLECTOR] å¼€å¯å…¨é‡æ”¶å‰²ï¼Œåªæ±‚æ•°é‡ï¼Œåç¼€æ³¨å…¥ä¸­...")
    targets = get_huge_sources()
    all_found = []
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = executor.map(fetch_nodes, targets)
        for res in results:
            if res: all_found.extend(res)

    unique_nodes = list(set(all_found))
    suffix = "youtube@å…è´¹å¼€æº"
    
    # æ‰“æ ‡ï¼šä¸ç®¡å¥½åï¼Œå…¨éƒ¨è´´ä¸Šè€å¤§çš„æ ‡
    final_nodes = [f"{n.split('#')[0]}#{suffix}" for n in unique_nodes]

    if final_nodes:
        with open("nodes.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(final_nodes))
        print(f"âœ… [DONE] æˆ˜æŠ¥ï¼šæ•è· {len(final_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œä»“åº“å·²æ›´æ–°ã€‚")
    else:
        print("âŒ è­¦å‘Šï¼šæœªå‘ç°èŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚")

if __name__ == "__main__":
    collector()
