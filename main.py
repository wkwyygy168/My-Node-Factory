import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def universal_extractor(url):
    """åƒå¸å°˜å™¨ä¸€æ ·ï¼Œæ— è§†æ ¼å¼ï¼Œåªå¸å–æœ‰æ•ˆçš„èŠ‚ç‚¹æŒ‡çº¹"""
    headers = {'User-Agent': 'clash.meta'}
    try:
        r = requests.get(url, headers=headers, timeout=20)
        if r.status_code != 200: return []
        
        raw_text = r.text
        # ç¬¬ä¸€æ­¥ï¼šæš´åŠ›æå–æ‰€æœ‰å¯è§çš„èŠ‚ç‚¹é“¾æ¥
        # å…è®¸åŒ…å«æ‰€æœ‰éç©ºç™½å­—ç¬¦ï¼Œç›´åˆ°é‡åˆ°å¼•å·ã€å°–æ‹¬å·æˆ–ç©ºæ ¼ç»“æŸï¼Œç¡®ä¿ä¸æˆªæ–­å‚æ•°
        pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic|http|https|socks5|socks)://[^\s<>"\']+'
        found = re.findall(pattern, raw_text, re.I)
        
        # ç¬¬äºŒæ­¥ï¼šå¯¹å…¨æ–‡è¿›è¡Œâ€œæ–­ç‚¹å¼â€Base64 å°è¯•
        # å¾ˆå¤š YAML ä¼šæŠŠ Base64 èŠ‚ç‚¹åŒ…åœ¨ç‰¹å®šå­—æ®µé‡Œï¼Œæˆ‘ä»¬ç›´æ¥æ‰«æå…¨æ–‡æœ¬ä¸­å¯èƒ½çš„ B64 å—
        b64_blocks = re.findall(r'[A-Za-z0-9+/]{40,}', raw_text)
        for block in b64_blocks:
            try:
                # è¡¥å…¨å¡«å……å¹¶å°è¯•è§£ç 
                missing_padding = len(block) % 4
                if missing_padding: block += "=" * (4 - missing_padding)
                decoded = base64.b64decode(block).decode('utf-8', errors='ignore')
                found.extend(re.findall(pattern, decoded, re.I))
            except:
                continue
        return found
    except:
        return []

def collector():
    print("ğŸš€ [GOD-COLLECTOR] æ­£åœ¨æ‰§è¡Œå…¨ç½‘æœ€å¼ºæš´åŠ›æ”¶å‰²ï¼Œç›®æ ‡ 92+ èŠ‚ç‚¹...")
    
    # é”å®šä½ çš„æ ¸å¿ƒé»„é‡‘æº
    targets = [
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/base64.txt",
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/all.yaml"
    ]

    all_raw_found = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(universal_extractor, targets)
        for res in results:
            if res: all_raw_found.extend(res)

    # é‡ç‚¹ï¼šå»é‡æ—¶å¿…é¡»ä¿ç•™åŸå§‹ç¼–ç ï¼Œé˜²æ­¢ TWÂ² ç­‰ç‰¹æ®Šç¬¦å·è¢«ç ´å
    unique_nodes = []
    seen = set()
    for node in all_raw_found:
        # å»æ‰æœ«å°¾å¯èƒ½è¢«è¯¯æŠ“çš„æ ‡ç‚¹ç¬¦å·
        clean_node = node.strip().rstrip(',').rstrip(';').rstrip('}')
        if clean_node and clean_node not in seen:
            unique_nodes.append(clean_node)
            seen.add(clean_node)
    
    # æŒ‰ç…§ä½ çš„éœ€æ±‚ï¼Œåˆå¹¶å¹¶è¾“å‡ºåˆ° nodes.txt
    with open("nodes.txt", "w", encoding="utf-8") as f:
        if unique_nodes:
            f.write("\n".join(unique_nodes))
            print(f"âœ… [SUCCESS] ä»»åŠ¡å®Œæˆï¼å…±è®¡å‡†ç¡®æ”¶é›† {len(unique_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        else:
            print("âŒ è­¦å‘Šï¼šæœªå‘ç°æœ‰æ•ˆèŠ‚ç‚¹ã€‚")

if __name__ == "__main__":
    collector()
