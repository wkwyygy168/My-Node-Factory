import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def fetch_pure_nodes(url):
    """æœ€å¼ºæ¬è¿é€»è¾‘ï¼šæ”¯æŒä¸Šæ ‡ã€ä¸‹æ ‡åŠæ‰€æœ‰ç‰¹æ®Šç¼–ç å­—ç¬¦"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    try:
        r = requests.get(url, headers=headers, timeout=25)
        if r.status_code == 200:
            raw_data = r.text.strip()
            # ã€æ ¸å¿ƒè¿›åŒ–ã€‘æ­£åˆ™è¡¨è¾¾å¼ï¼š
            # 1. å…è®¸åè®®å¤´åŒ…å«ç‰¹æ®Šå­—ä½“
            # 2. åŒ¹é…èŒƒå›´æ‰©å¤§åˆ°éç©ºå­—ç¬¦é›†ï¼Œç¡®ä¿ä¸è¢«ä¸Šæ ‡â€œ2â€ç­‰ç‰¹æ®Šç¬¦å·æˆªæ–­
            pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic|http|https|socks5|socks|wireguard)://[^\s<>"]+'
            
            # ç›´æ¥æå–åŸå§‹æ˜æ–‡
            found = re.findall(pattern, raw_data, re.I)
            
            # é’ˆå¯¹ Base64 çš„æ·±åº¦æ¸…æ´—æå–
            try:
                # åªä¿ç•™ Base64 åˆæ³•å­—ç¬¦ç”¨äºè§£ç 
                b64_only = re.sub(r'[^A-Za-z0-9+/=]', '', raw_data)
                missing_padding = len(b64_only) % 4
                if missing_padding:
                    b64_only += "=" * (4 - missing_padding)
                decoded = base64.b64decode(b64_only).decode('utf-8', errors='ignore')
                # åœ¨è§£ç åçš„å†…å®¹é‡Œå†æ¬¡è¿›è¡Œå…¨é‡æ‰«æ
                found.extend(re.findall(pattern, decoded, re.I))
            except:
                pass
            return found
    except:
        return []

def collector():
    print("ğŸš€ [ULTIMATE-RADAR] æ­£åœ¨é€šè¿‡â€˜å…¨å­—ç¬¦è¯†åˆ«â€™æ‰¾å›å¤±è¸ªçš„å°æ¹¾èŠ‚ç‚¹...")
    
    targets = [
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/base64.txt",
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/all.yaml"
    ]

    all_found = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_pure_nodes, targets)
        for res in results:
            if res:
                all_found.extend(res)

    # æ·±åº¦å»é‡ï¼šä¿ç•™æœ€åŸå§‹çš„ç¼–ç ï¼Œä¸è¿›è¡Œä»»ä½•å­—ç¬¦è½¬æ¢
    unique_nodes = []
    seen = set()
    for node in all_found:
        # ä½¿ç”¨ strip() æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ¢è¡Œå¹²æ‰°ï¼Œä½†ä¿ç•™åè®®å†…çš„æ‰€æœ‰ç‰¹æ®Šç¬¦å·
        node_clean = node.strip()
        if node_clean and node_clean not in seen:
            unique_nodes.append(node_clean)
            seen.add(node_clean)
    
    with open("nodes.txt", "w", encoding="utf-8") as f:
        if unique_nodes:
            # å…³é”®ï¼šä»¥ UTF-8 ç¼–ç å†™å…¥ï¼Œç¡®ä¿ $TW^2$ ç­‰ç‰¹æ®Šç¬¦å·ä¸ä¹±ç 
            f.write("\n".join(unique_nodes))
            print(f"âœ… [DONE] æ¬è¿æˆåŠŸï¼å½“å‰å…±è®¡ï¼š{len(unique_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
            print(f"ğŸ“Š æç¤ºï¼šå·²é’ˆå¯¹ä¸Šæ ‡å­—ç¬¦ï¼ˆå¦‚ TW^2ï¼‰å®Œæˆç¼–ç ä¼˜åŒ–ã€‚")
        else:
            print("âŒ æœªå‘ç°èŠ‚ç‚¹ã€‚")

if __name__ == "__main__":
    collector()
