import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def fetch_content(url):
    # æ›´åŠ ç¨³å¥çš„é•œåƒè½¬æ¢ï¼Œé˜²æ­¢è„šæœ¬å›  URL é”™è¯¯è€Œæ‰“å‰
    alt_url = url
    if "raw.githubusercontent.com" in url:
        try:
            # å…¼å®¹å¤šç§åˆ†æ”¯å‘½åçš„è½¬æ¢é€»è¾‘
            parts = url.split('/')
            if len(parts) >= 5:
                user, repo = parts[3], parts[4]
                path = "/".join(parts[6:])
                branch = parts[5]
                alt_url = f"https://fastly.jsdelivr.net/gh/{user}/{repo}@{branch}/{path}"
        except: alt_url = url # è½¬æ¢å¤±è´¥åˆ™ç”¨åŸé“¾æ¥

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    # å°è¯•åŒè·¯å¾„æŠ“å–ï¼šå…ˆé•œåƒï¼ŒååŸå§‹
    for target_url in [alt_url, url]:
        try:
            r = requests.get(target_url, headers=headers, timeout=12)
            if r.status_code == 200 and len(r.text) > 50:
                content = r.text.strip()
                pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
                
                # 1. æ˜æ–‡åŒ¹é…
                nodes = re.findall(pattern, content, re.I)
                
                # 2. æš´åŠ›è§£ç  (è§£å†³ 1 ä¸ªèŠ‚ç‚¹çš„æ ¸å¿ƒ)
                try:
                    b64_str = re.sub(r'[^a-zA-Z0-9+/=]', '', content)
                    missing_padding = len(b64_str) % 4
                    if missing_padding: b64_str += "=" * (4 - missing_padding)
                    decoded = base64.b64decode(b64_str).decode('utf-8', errors='ignore')
                    nodes.extend(re.findall(pattern, decoded, re.I))
                except: pass
                
                if nodes: return nodes
        except: continue
    return []

def collector():
    print("ğŸš€ [SYSTEM] å¼•æ“ V10.1ï¼šå¯åŠ¨é•œåƒ+åŸå§‹åŒè·¯å¾„çˆ†ç ´æ¨¡å¼...")
    # è¿™é‡Œæ”¾ä½ é‚£ 80 æ¡æº
    targets = [ "è¿™é‡Œæ˜¯ä½ çš„80æ¡é“¾æ¥åˆ—è¡¨..." ] 

    all_found = []
    # ä¿æŒä¸­é€Ÿå¹¶å‘ï¼Œç¡®ä¿ç¨³å®šæ€§
    with ThreadPoolExecutor(max_workers=15) as executor:
        results = executor.map(fetch_content, targets)
        for res in results:
            if res: all_found.extend(res)

    unique_nodes = list(set(all_found))
    with open("nodes.txt", "w", encoding="utf-8") as f:
        if len(unique_nodes) > 2:
            f.write("\n".join(unique_nodes))
            print(f"âœ… [SUCCESS] æˆåŠŸæ”¶å‰²å”¯ä¸€èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
        else:
            # å“ªæ€•åªæœ‰ä¿åº•ï¼Œä¹Ÿç»ä¸è®©æ–‡ä»¶ä¸ºç©ºå¯¼è‡´ Karing æŠ¥é”™
            f.write("ss://YWVzLTI1Ni1jZmI6WG44aktkbURNMDBJZU8lIyQjZkpBTXRzRUFFVU9wSC9ZV1l0WXFERm5UMFNWQDEwMy4xODYuMTU1LjI3OjM4Mzg4#å¼•æ“åŒè·¯å°è¯•ä¸­_ç¨ååˆ·æ–°")

if __name__ == "__main__":
    collector()
