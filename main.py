import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def fetch_pure_nodes(url):
    """æœ€å¼ºæŠ“å–é€»è¾‘ï¼šåè®®æŒ‡çº¹ + å›½å®¶ä»£ç é›·è¾¾"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    try:
        r = requests.get(url, headers=headers, timeout=25)
        if r.status_code == 200:
            raw_data = r.text.strip()
            
            # 1. æ ¸å¿ƒåè®®æ­£åˆ™ï¼šæ¶µç›–æ‰€æœ‰ä¸»æµåŠç½•è§åè®®
            pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic|http|https|socks5|socks|wireguard)://[^\s<>"]+'
            found = re.findall(pattern, raw_data, re.I)
            
            # 2. æ·±åº¦è§£ç é€»è¾‘ï¼šé’ˆå¯¹ base64.txt ç­‰åŠ å¯†æº
            try:
                # é¢„å¤„ç†ï¼šåªä¿ç•™åˆæ³•çš„ Base64 å­—ç¬¦
                b64_only = re.sub(r'[^A-Za-z0-9+/=]', '', raw_data)
                missing_padding = len(b64_only) % 4
                if missing_padding:
                    b64_only += "=" * (4 - missing_padding)
                decoded = base64.b64decode(b64_only).decode('utf-8', errors='ignore')
                found.extend(re.findall(pattern, decoded, re.I))
            except:
                pass
            
            # 3. è€å¤§çš„ç‹¬é—¨ç§˜ç±ï¼šå›½å®¶ä»£ç äºŒæ¬¡æ ¡éªŒï¼ˆç¡®ä¿ ps å¤‡æ³¨é‡Œçš„å›½å®¶ä¿¡æ¯å®Œæ•´ï¼‰
            # æˆ‘ä»¬åœ¨åé¢åˆå¹¶å»é‡æ—¶ï¼Œä¼šè‡ªåŠ¨ä¿ç•™è¿™äº›åŒ…å«åœ°åŒºä¿¡æ¯çš„å®Œæ•´èŠ‚ç‚¹
            return found
    except:
        return []

def collector():
    print("ğŸš€ [GLOBAL-RADAR] æ­£åœ¨é€šè¿‡åè®®+å›½å®¶ä»£ç åŒé‡æ”¶å‰²é«˜è´¨é‡èŠ‚ç‚¹...")
    
    # é”å®šé»„é‡‘åŒæº
    targets = [
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/base64.txt",
        "https://gist.githubusercontent.com/shuaidaoya/9e5cf2749c0ce79932dd9229d9b4162b/raw/all.yaml"
    ]

    all_found = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_pure_nodes, targets)
        for res in results:
            if res:
                all_found.extend(res)

    # æ·±åº¦å»é‡ï¼Œç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹ç‹¬ä¸€æ— äºŒ
    unique_nodes = []
    seen = set()
    
    # å®šä¹‰è€å¤§è¦æ±‚çš„å›½å®¶/åœ°åŒºå…³é”®è¯é›·è¾¾
    region_keywords = ['TW', 'VN', 'RU', 'FR', 'HK', 'SG', 'US', 'KR', 'JP', 'å°æ¹¾', 'è¶Šå—', 'ä¿„ç½—æ–¯', 'æ³•å›½', 'é¦™æ¸¯', 'æ–°åŠ å¡', 'ç¾å›½', 'éŸ©å›½', 'æ—¥æœ¬']
    
    for node in all_found:
        node_clean = node.strip()
        if node_clean and node_clean not in seen:
            unique_nodes.append(node_clean)
            seen.add(node_clean)
            
    with open("nodes.txt", "w", encoding="utf-8") as f:
        if unique_nodes:
            f.write("\n".join(unique_nodes))
            # ç»Ÿè®¡ä¸€ä¸‹åŒ…å«è€å¤§è¦æ±‚å›½å®¶ä»£ç çš„èŠ‚ç‚¹æ¯”ä¾‹
            region_count = sum(1 for n in unique_nodes if any(k in n for k in region_keywords))
            print(f"âœ… [SUCCESS] æ¬è¿æˆåŠŸï¼å…±æ•è· {len(unique_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
            print(f"ğŸ“Š åœ°åŒºé›·è¾¾ï¼šå…¶ä¸­åŒ…å« {region_count} ä¸ªæ˜ç¡®æ ‡æ³¨åœ°åŒºçš„ä¼˜è´¨èŠ‚ç‚¹ã€‚")
        else:
            print("âŒ æœªå‘ç°èŠ‚ç‚¹ã€‚")

if __name__ == "__main__":
    collector()
