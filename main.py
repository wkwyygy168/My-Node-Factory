import requests
import re
import base64
from concurrent.futures import ThreadPoolExecutor

def fetch_content(url):
    # è‡ªåŠ¨è½¬æ¢ GitHub é“¾æ¥ä¸ºé•œåƒé“¾æ¥ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
    if "raw.githubusercontent.com" in url:
        url = url.replace("raw.githubusercontent.com", "fastly.jsdelivr.net/gh").replace("/master/", "@master/").replace("/main/", "@main/")
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            content = r.text.strip()
            # åè®®æŒ‡çº¹
            pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
            
            # 1. ç›´æ¥æå–æ˜æ–‡
            nodes = re.findall(pattern, content, re.I)
            
            # 2. æš´åŠ› Base64 è§£ç é€»è¾‘
            try:
                # è¡¥å…¨å¡«å……ç¬¦å¹¶æ¸…æ´—éæ³•å­—ç¬¦
                b64_str = re.sub(r'[^a-zA-Z0-9+/=]', '', content)
                padding = len(b64_str) % 4
                if padding: b64_str += "=" * (4 - padding)
                decoded = base64.b64decode(b64_str).decode('utf-8', errors='ignore')
                nodes.extend(re.findall(pattern, decoded, re.I))
            except: pass
            
            return nodes
    except: return []

def collector():
    print("ğŸš€ [SYSTEM] å¼•æ“ V10.0ï¼šæ­£åœ¨é€šè¿‡ CDN é•œåƒè¿›è¡Œå…¨é‡çˆ†ç ´...")
    
    # è¿™é‡Œçš„ targets å»ºè®®å…ˆç”¨ä½ æœ€ä¿¡ä»»çš„ 10 ä¸ªè¯•è¯•ï¼Œå¦‚æœé€šäº†å†åŠ åˆ° 80 ä¸ª
    targets = [
        "https://raw.githubusercontent.com/freefq/free/master/v2ray",
        "https://raw.githubusercontent.com/vpei/free-node/master/v2ray.txt",
        "https://raw.githubusercontent.com/Pawpieee/Free-Proxies/main/sub/sub_merge.txt",
        "https://raw.githubusercontent.com/anaer/Sub/master/v2ray.txt"
        # ... (æ­¤å¤„ä¿æŒä½ åŸæœ¬çš„ 80 æ¡åˆ—è¡¨å³å¯)
    ]

    all_found = []
    # é™ä½å¹¶å‘åˆ° 10ï¼Œç»†æ°´é•¿æµé˜²æ­¢è¢«å°
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_content, targets)
        for res in results:
            if res: all_found.extend(res)

    unique_nodes = list(set(all_found))
    with open("nodes.txt", "w", encoding="utf-8") as f:
        if len(unique_nodes) > 1:
            f.write("\n".join(unique_nodes))
            print(f"âœ… [SUCCESS] çˆ†ç ´å®Œæˆï¼æˆåŠŸæ”¶å‰²èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
        else:
            # ä¿®æ”¹ä¿åº•ä¿¡æ¯ï¼Œç¡®ä¿ä¸è®© Karing æŠ¥ç©º
            f.write("ss://YWVzLTI1Ni1jZmI6WG44aktkbURNMDBJZU8lIyQjZkpBTXRzRUFFVU9wSC9ZV1l0WXFERm5UMFNWQDEwMy4xODYuMTU1LjI3OjM4Mzg4#äº‘ç«¯æ”¶å‰²æœºæ­£åœ¨å…¨åŠ›ä½œä¸š_è¯·ç¨ååˆ·æ–°")

if __name__ == "__main__":
    collector()
