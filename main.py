import requests
import re
import base64
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

def fetch_and_decode(url):
    """å·¥ä¸šçº§æ”¶å‰²å¼•æ“ï¼šä¼ªè£…ã€å¼ºå–ã€è§£ç """
    # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«æºç«™æ‹¦æˆª
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/plain,text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'
    }
    try:
        r = requests.get(url, headers=headers, timeout=20)
        if r.status_code == 200:
            content = r.text.strip()
            # åè®®è¯†åˆ«æ­£åˆ™ï¼šæ›´å…¨é¢åœ°åŒ¹é…å„ç±»æ ¼å¼
            pattern = r'(?:ss|ssr|vmess|vless|trojan|hy2|tuic)://[^\s<>"]+'
            
            # 1. æå–æ˜æ–‡
            found = re.findall(pattern, content, re.I)
            
            # 2. å°è¯•å¤šå±‚ Base64 è§£ç æå–
            temp_content = content
            for _ in range(3): # å¢åŠ åˆ°3å±‚æ·±åº¦ï¼Œå½»åº•å¸å¹²éšè—èŠ‚ç‚¹
                try:
                    missing_padding = len(temp_content) % 4
                    if missing_padding: temp_content += "=" * (4 - missing_padding)
                    decoded = base64.b64decode(temp_content).decode('utf-8', errors='ignore')
                    found.extend(re.findall(pattern, decoded, re.I))
                    temp_content = decoded # é€’å½’å‘ä¸‹
                except: break
            return found
    except: return []

def get_dynamic_urls():
    """å…¨è‡ªåŠ¨æ—¥æœŸæ¨ç®—ï¼šå¯¹é½ Barabama é€»è¾‘"""
    dynamic_list = []
    today = datetime.now()
    for i in range(10): # æ‰©å¤§åˆ°æœ€è¿‘10å¤©ï¼Œç¡®ä¿æ¯å¤©éƒ½æœ‰æ–°è´§
        t = today - timedelta(days=i)
        d_str, m_str, y_str = t.strftime("%Y%m%d"), t.strftime("%m"), t.strftime("%Y")
        dynamic_list.append(f"https://node.nodefree.me/{y_str}/{m_str}/{d_str}.txt")
        dynamic_list.append(f"https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/{d_str}.txt")
    return dynamic_list

def collector():
    print("ğŸš€ [POWER-FACTORY] å¼•æ“å…¨åŠŸç‡å¼€å¯ï¼Œæ­£åœ¨æ¨ªæ‰«å…¨ç½‘èµ„æº...")
    
    # ç²¾é€‰ä¸¤å¤§å®¶æœ€å¼ºæº + ä½ çš„ 80+ åŸºç¡€æº
    targets = list(set([
        *get_dynamic_urls(),
        "https://raw.githubusercontent.com/shuaidaoya/FreeNodes/main/nodes.txt",
        "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/nodes.txt",
        "https://raw.githubusercontent.com/vpei/Free-Node-Merge/main/o/node.txt",
        "https://raw.githubusercontent.com/tjm022/Free-Node-Merge/main/node.txt",
        "https://raw.githubusercontent.com/mizero/FreeNode/main/nodes.txt",
        "https://t.me/s/v2rayfree",
        "https://t.me/s/V2List",
        "https://t.me/s/daily_free_nodes",
        # è¿™é‡Œå»ºè®®ç»§ç»­ä¿ç•™ä½ åŸæœ¬å¥½ç”¨çš„é‚£å‡ åä¸ªé“¾æ¥
    ]))

    all_found = []
    # 50 çº¿ç¨‹æé€Ÿçªå›´
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = executor.map(fetch_and_decode, targets)
        for res in results:
            if res: all_found.extend(res)

    # æ·±åº¦å»é‡
    unique_nodes = list(set(all_found))
    
    # --- æ ¸å¿ƒæ‰“æ ‡é€»è¾‘ ---
    final_nodes = []
    suffix = "youtube@å…è´¹å¼€æº"
    for node in unique_nodes:
        # æ¸…ç†æ—§å¤‡æ³¨ï¼Œæ³¨å…¥è€å¤§ä¸“å±æ ‡
        base_node = node.split("#")[0]
        final_nodes.append(f"{base_node}#{suffix}")

    # --- æ”¹è¿›åçš„å†™å…¥é€»è¾‘ï¼šç§»é™¤ä¿åº•æœºåˆ¶ï¼ŒæŠ“åˆ°å¤šå°‘å†™å¤šå°‘ ---
    if final_nodes:
        with open("nodes.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(final_nodes))
        print(f"âœ… [SUCCESS] æˆ˜æœï¼šæ•è· {len(final_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œåç¼€å·²æ³¨å…¥ã€‚")
    else:
        print("âŒ [FAILED] æœ¬æ¬¡æœªæŠ“åˆ°æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œç¯å¢ƒã€‚")

if __name__ == "__main__":
    collector()
